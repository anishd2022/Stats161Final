{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import necessary libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
        "from sklearn.metrics import (\n",
        "    confusion_matrix, precision_score, recall_score, accuracy_score, f1_score,\n",
        "    roc_auc_score, average_precision_score\n",
        ")\n",
        "import lightgbm as lgb\n",
        "import xgboost as xgb\n",
        "import re\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# For generative modeling\n",
        "try:\n",
        "    from sdv.metadata import SingleTableMetadata\n",
        "    from sdv.single_table import CTGANSynthesizer, TVAESynthesizer\n",
        "    SDV_AVAILABLE = True\n",
        "except ImportError:\n",
        "    print(\"SDV library not found. Please install it using: pip install sdv\")\n",
        "    SDV_AVAILABLE = False\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Helper functions (from c161.ipynb)\n",
        "_splitter = re.compile(r\"[,;\\^\\s]+\")\n",
        "\n",
        "def count_listish(x):\n",
        "    \"\"\"Count items in common 'list-like' encodings without eval.\"\"\"\n",
        "    if x is None or (isinstance(x, float) and pd.isna(x)): return 0\n",
        "    s = str(x).strip()\n",
        "    if s == \"\" or s == \"[]\": return 0\n",
        "    if s.startswith(\"[\") and s.endswith(\"]\"):\n",
        "        s = s[1:-1]\n",
        "    parts = [p for p in _splitter.split(s) if p]\n",
        "    return len(parts)\n",
        "\n",
        "def eval_at_threshold(y_true, y_prob, thr=0.5, label=\"Model\"):\n",
        "    y_pred = (y_prob >= thr).astype(int)\n",
        "    cm = confusion_matrix(y_true, y_pred, labels=[0,1])\n",
        "    precision = precision_score(y_true, y_pred, zero_division=0)\n",
        "    recall    = recall_score(y_true, y_pred, zero_division=0)\n",
        "    acc       = accuracy_score(y_true, y_pred)\n",
        "    f1        = f1_score(y_true, y_pred, zero_division=0)\n",
        "    roc_auc   = roc_auc_score(y_true, y_prob)\n",
        "    pr_auc    = average_precision_score(y_true, y_prob)\n",
        "    print(f\"\\n{label} @ threshold={thr:.3f}\")\n",
        "    print(\"Confusion Matrix:\\n\", cm)\n",
        "    print(f\"Precision: {precision:.4f}, Recall: {recall:.4f}, \"\n",
        "          f\"Accuracy: {acc:.4f}, F1: {f1:.4f}, ROC-AUC: {roc_auc:.4f}, PR-AUC: {pr_auc:.4f}\")\n",
        "    return {\"thr\": thr, \"precision\": precision, \"recall\": recall, \"accuracy\": acc,\n",
        "            \"f1\": f1, \"roc_auc\": roc_auc, \"pr_auc\": pr_auc}\n",
        "\n",
        "def best_threshold_by_f1(y_true, y_prob, grid=None):\n",
        "    if grid is None:\n",
        "        grid = np.linspace(0.02, 0.50, 49)  # focus on lower thresholds for rare positives\n",
        "    best = (-1, 0.5)\n",
        "    for t in grid:\n",
        "        f1 = f1_score(y_true, (y_prob >= t).astype(int), zero_division=0)\n",
        "        if f1 > best[0]:\n",
        "            best = (f1, t)\n",
        "    return best[1]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Generative AI Analysis for Balanced Synthetic Data\n",
        "\n",
        "## Goal\n",
        "Generate synthetic data that replicates the real dataset but with balanced classes (50% clicked, 50% no-click) instead of the current 2% vs 98% imbalance.\n",
        "\n",
        "## Approach\n",
        "1. Load and preprocess data (similar to midterm project)\n",
        "2. Train a generative model (CTGAN or TVAE) on the real data\n",
        "3. Generate synthetic data with balanced classes\n",
        "4. Train XGBoost and LightGBM on synthetic data\n",
        "5. Evaluate on the same test set as midterm project\n",
        "6. Compare results with midterm project metrics\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading data...\n",
            "Data shape: (7675517, 79)\n",
            "Columns: ['user_id', 'log_id', 'label', 'age', 'gender', 'residence', 'city', 'city_rank', 'series_dev', 'series_group']...\n"
          ]
        }
      ],
      "source": [
        "# Step 1: Load Data (same as c161.ipynb)\n",
        "print(\"Loading data...\")\n",
        "train_merged = pd.read_csv(\"ads_train_enriched.csv\")\n",
        "train_merged = train_merged.drop(columns=['u_userId'], errors='ignore')\n",
        "\n",
        "print(f\"Data shape: {train_merged.shape}\")\n",
        "print(f\"Columns: {train_merged.columns.tolist()[:10]}...\")  # Show first 10 columns\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Preparing data for generative modeling...\n",
            "Data shape for generation: (7675517, 77)\n",
            "Class distribution in real data:\n",
            "  Positives (1): 119,136 (1.55%)\n",
            "  Negatives (0): 7,556,381 (98.45%)\n"
          ]
        }
      ],
      "source": [
        "# Step 2: Prepare data for generative modeling\n",
        "print(\"Preparing data for generative modeling...\")\n",
        "target = 'label'\n",
        "X = train_merged.drop(columns=[target], errors='ignore')\n",
        "y = train_merged[target] if target in train_merged.columns else None\n",
        "\n",
        "# Drop unnecessary columns\n",
        "for col in ['pt_d', 'log_id']:\n",
        "    if col in X.columns:\n",
        "        X = X.drop(columns=[col])\n",
        "\n",
        "# Remove object columns\n",
        "obj_cols = X.select_dtypes(include=['object']).columns\n",
        "if len(obj_cols) > 0:\n",
        "    X = X.drop(columns=obj_cols)\n",
        "\n",
        "# Combine features and target for generative model\n",
        "data_for_generation = X.copy()\n",
        "data_for_generation['label'] = y\n",
        "\n",
        "print(f\"Data shape for generation: {data_for_generation.shape}\")\n",
        "print(f\"Class distribution in real data:\")\n",
        "print(f\"  Positives (1): {(y == 1).sum():,} ({(y == 1).mean()*100:.2f}%)\")\n",
        "print(f\"  Negatives (0): {(y == 0).sum():,} ({(y == 0).mean()*100:.2f}%)\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Checking data quality and memory...\n",
            "Data shape: (7675517, 77)\n",
            "Memory usage: 4509.09 MB\n",
            "\n",
            "Checking for problematic values...\n",
            "\n",
            "Label distribution:\n",
            "  Positives (1): 119,136 (1.55%)\n",
            "  Negatives (0): 7,556,381 (98.45%)\n",
            "\n",
            "Data quality check completed!\n"
          ]
        }
      ],
      "source": [
        "# Step 3.5: Data quality check and memory diagnostics\n",
        "print(\"Checking data quality and memory...\")\n",
        "print(f\"Data shape: {data_for_generation.shape}\")\n",
        "print(f\"Memory usage: {data_for_generation.memory_usage(deep=True).sum() / 1024**2:.2f} MB\")\n",
        "\n",
        "# Check for problematic columns\n",
        "print(\"\\nChecking for problematic values...\")\n",
        "numeric_cols = data_for_generation.select_dtypes(include=[np.number]).columns\n",
        "problematic_cols = []\n",
        "for col in numeric_cols:\n",
        "    if data_for_generation[col].isna().sum() > len(data_for_generation) * 0.5:\n",
        "        problematic_cols.append(col)\n",
        "        print(f\"  {col}: {data_for_generation[col].isna().sum():,} NaN values ({data_for_generation[col].isna().mean()*100:.1f}%)\")\n",
        "\n",
        "if problematic_cols:\n",
        "    print(f\"\\nWarning: {len(problematic_cols)} columns have >50% NaN values\")\n",
        "    print(\"Consider dropping these columns or handling them separately\")\n",
        "\n",
        "# Check label distribution in sample\n",
        "print(f\"\\nLabel distribution:\")\n",
        "print(f\"  Positives (1): {(data_for_generation['label'] == 1).sum():,} ({(data_for_generation['label'] == 1).mean()*100:.2f}%)\")\n",
        "print(f\"  Negatives (0): {(data_for_generation['label'] == 0).sum():,} ({(data_for_generation['label'] == 0).mean()*100:.2f}%)\")\n",
        "\n",
        "print(\"\\nData quality check completed!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Troubleshooting Kernel Crashes\n",
        "\n",
        "If the kernel crashes during generative model training, common causes include:\n",
        "\n",
        "1. **Memory Issues**: CTGAN/TVAE can be memory-intensive with many features\n",
        "2. **High Dimensionality**: 77 features can be challenging for generative models\n",
        "3. **Data Type Issues**: Some data types may not be compatible\n",
        "\n",
        "**Solutions implemented:**\n",
        "- Uses TVAE first (more stable than CTGAN for high-dimensional data)\n",
        "- Reduced batch sizes and epochs\n",
        "- Data quality checks and cleaning\n",
        "- Fallback to smaller samples if needed\n",
        "- Error handling with graceful degradation\n",
        "\n",
        "**If it still crashes, try:**\n",
        "- Reduce sample_size further (e.g., 2000-3000)\n",
        "- Use feature selection to reduce dimensionality\n",
        "- Train on separate class subsets and combine\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Important: Training on Balanced Data\n",
        "\n",
        "**For better synthetic data generation**, consider training the generative model on a balanced subset of the real data. This helps the model learn both classes equally and generate balanced synthetic data more easily.\n",
        "\n",
        "The code below will automatically try this if the initial generation fails, but you can also do it proactively.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 800x500 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Class imbalance ratio: 63.43:1\n"
          ]
        }
      ],
      "source": [
        "# Step 3: Visualize class imbalance\n",
        "fig, ax = plt.subplots(1, 1, figsize=(8, 5))\n",
        "class_counts = pd.Series({'No Click (0)': (y == 0).sum(), 'Click (1)': (y == 1).sum()})\n",
        "sns.barplot(x=class_counts.index, y=class_counts.values, ax=ax, palette='pastel')\n",
        "ax.set_title(\"Class Distribution in Real Data\")\n",
        "ax.set_ylabel(\"Sample Count\")\n",
        "for i, v in enumerate(class_counts.values):\n",
        "    ax.text(i, v, f\"{v:,}\", ha='center', va='bottom', fontsize=10)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(f\"\\nClass imbalance ratio: {(y == 0).sum() / (y == 1).sum():.2f}:1\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Alternative: Feature Selection Approach\n",
        "\n",
        "If the full feature set causes memory issues, you can use feature selection to reduce dimensionality before training the generative model.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Alternative Step 4: Feature Selection (Use if full model crashes)\n",
        "# Uncomment and run this cell if the generative model training keeps crashing\n",
        "\n",
        "USE_FEATURE_SELECTION = False  # Set to True if you need to reduce dimensionality\n",
        "\n",
        "if USE_FEATURE_SELECTION and SDV_AVAILABLE:\n",
        "    from sklearn.feature_selection import SelectKBest, f_classif\n",
        "    \n",
        "    print(\"Using feature selection to reduce dimensionality...\")\n",
        "    \n",
        "    # Prepare data\n",
        "    X_for_selection = data_for_generation.drop(columns=['label'])\n",
        "    y_for_selection = data_for_generation['label']\n",
        "    \n",
        "    # Select top 30 features (adjust as needed)\n",
        "    k_best = 30\n",
        "    selector = SelectKBest(score_func=f_classif, k=k_best)\n",
        "    X_selected = selector.fit_transform(X_for_selection, y_for_selection)\n",
        "    \n",
        "    # Get selected feature names\n",
        "    selected_features = X_for_selection.columns[selector.get_support()].tolist()\n",
        "    print(f\"Selected {len(selected_features)} features out of {len(X_for_selection.columns)}\")\n",
        "    print(f\"Selected features: {selected_features[:10]}...\")  # Show first 10\n",
        "    \n",
        "    # Create reduced dataset\n",
        "    data_for_generation_reduced = pd.DataFrame(X_selected, columns=selected_features)\n",
        "    data_for_generation_reduced['label'] = y_for_selection.values\n",
        "    \n",
        "    # Update data_for_generation to use reduced version\n",
        "    data_for_generation = data_for_generation_reduced.copy()\n",
        "    print(f\"Reduced data shape: {data_for_generation.shape}\")\n",
        "    \n",
        "    print(\"\\nNow proceed with Step 4 using the reduced dataset...\")\n",
        "else:\n",
        "    print(\"Feature selection not enabled. Using full feature set.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training generative model...\n",
            "Note: This may take a while depending on data size...\n",
            "Using sample of 5,000 rows for training...\n",
            "Checking data quality...\n",
            "Data sample shape: (5000, 77)\n",
            "Data types: int64      72\n",
            "float64     5\n",
            "Name: count, dtype: int64\n",
            "Creating metadata...\n",
            "\n",
            "Attempting to train TVAE (more stable for high-dimensional data)...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Loss: 0.000:   0%|          | 0/50 [00:00<?, ?it/s]"
          ]
        },
        {
          "ename": "",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
            "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
            "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
            "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
          ]
        }
      ],
      "source": [
        "\n",
        "# Step 4: Train generative model\n",
        "if not SDV_AVAILABLE:\n",
        "    print(\"SDV library is not available. Please install it first.\")\n",
        "else:\n",
        "    print(\"Training generative model...\")\n",
        "    print(\"Note: This may take a while depending on data size...\")\n",
        "    \n",
        "    # Sample a subset for faster training\n",
        "    # Start with a smaller sample to test, then increase if needed\n",
        "    sample_size = min(5000, len(data_for_generation))\n",
        "    # use the line below for full dataset\n",
        "    # sample_size = len(data_for_generation)\n",
        "    print(f\"Using sample of {sample_size:,} rows for training...\")\n",
        "    data_sample = data_for_generation.sample(n=sample_size, random_state=42).copy()\n",
        "    \n",
        "    # Ensure label is integer type (CTGAN can be sensitive to data types)\n",
        "    if 'label' in data_sample.columns:\n",
        "        data_sample['label'] = data_sample['label'].astype(int)\n",
        "    \n",
        "    # Check for infinite or very large values that might cause issues\n",
        "    print(\"Checking data quality...\")\n",
        "    numeric_cols = data_sample.select_dtypes(include=[np.number]).columns\n",
        "    for col in numeric_cols:\n",
        "        if np.isinf(data_sample[col]).any() or (np.abs(data_sample[col]) > 1e10).any():\n",
        "            print(f\"Warning: Column {col} has extreme values. Clipping...\")\n",
        "            data_sample[col] = data_sample[col].replace([np.inf, -np.inf], np.nan)\n",
        "            data_sample[col] = data_sample[col].fillna(data_sample[col].median())\n",
        "            data_sample[col] = np.clip(data_sample[col], -1e10, 1e10)\n",
        "    \n",
        "    # Fill any remaining NaN values\n",
        "    data_sample = data_sample.fillna(0)\n",
        "    \n",
        "    print(f\"Data sample shape: {data_sample.shape}\")\n",
        "    print(f\"Data types: {data_sample.dtypes.value_counts()}\")\n",
        "    \n",
        "    try:\n",
        "        # Create metadata\n",
        "        print(\"Creating metadata...\")\n",
        "        metadata = SingleTableMetadata()\n",
        "        metadata.detect_from_dataframe(data_sample)\n",
        "        \n",
        "        # Try TVAE first (often more stable than CTGAN for high-dimensional data)\n",
        "        print(\"\\nAttempting to train TVAE (more stable for high-dimensional data)...\")\n",
        "        try:\n",
        "            synthesizer = TVAESynthesizer(\n",
        "                metadata,\n",
        "                epochs=50,  # Start with fewer epochs\n",
        "                batch_size=500,  # Smaller batch size to reduce memory\n",
        "                verbose=True\n",
        "            )\n",
        "            synthesizer.fit(data_sample)\n",
        "            model_type = \"TVAE\"\n",
        "            print(\"TVAE training completed successfully!\")\n",
        "        except Exception as e:\n",
        "            print(f\"TVAE failed: {e}\")\n",
        "            print(\"\\nTrying CTGAN with optimized settings...\")\n",
        "            # Fallback to CTGAN with memory-efficient settings\n",
        "            synthesizer = CTGANSynthesizer(\n",
        "                metadata,\n",
        "                epochs=50,  # Reduced epochs\n",
        "                batch_size=500,  # Smaller batch size\n",
        "                generator_dim=(128, 128),  # Smaller generator\n",
        "                discriminator_dim=(128, 128),  # Smaller discriminator\n",
        "                verbose=True\n",
        "            )\n",
        "            synthesizer.fit(data_sample)\n",
        "            model_type = \"CTGAN\"\n",
        "            print(\"CTGAN training completed successfully!\")\n",
        "        \n",
        "    except Exception as e:\n",
        "        print(f\"Error during model training: {e}\")\n",
        "        print(\"\\nTrying with even smaller sample and simpler model...\")\n",
        "        # Try with smaller sample\n",
        "        small_sample = data_sample.sample(n=min(5000, len(data_sample)), random_state=42)\n",
        "        metadata = SingleTableMetadata()\n",
        "        metadata.detect_from_dataframe(small_sample)\n",
        "        \n",
        "        synthesizer = TVAESynthesizer(\n",
        "            metadata,\n",
        "            epochs=30,\n",
        "            batch_size=250,\n",
        "            verbose=True\n",
        "        )\n",
        "        synthesizer.fit(small_sample)\n",
        "        model_type = \"TVAE\"\n",
        "        data_sample = small_sample  # Update to use smaller sample\n",
        "        print(\"Training completed with smaller sample!\")\n",
        "    \n",
        "    print(f\"\\nGenerative model ({model_type}) training completed!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Step 5: Generate synthetic data with balanced classes\n",
        "if SDV_AVAILABLE:\n",
        "    print(\"Generating synthetic data with balanced classes...\")\n",
        "    print(\"Note: Since the model was trained on imbalanced data, we'll generate many samples\")\n",
        "    print(\"      and filter to get balanced classes...\")\n",
        "    \n",
        "    # Target number of samples per class (adjust as needed)\n",
        "    # For testing, use smaller numbers; for production, use larger\n",
        "    TARGET_SAMPLES_PER_CLASS = 50000  # Adjust based on your needs\n",
        "    # For quick testing, use: TARGET_SAMPLES_PER_CLASS = 1000\n",
        "    \n",
        "    print(f\"Target: {TARGET_SAMPLES_PER_CLASS:,} samples per class\")\n",
        "    \n",
        "    # Generate many more samples than needed to get enough positives\n",
        "    # Since positives are rare (~1.5%), we need to generate ~50-100x more samples\n",
        "    generation_multiplier = 50  # Generate 50x to ensure we get enough positives\n",
        "    samples_to_generate = TARGET_SAMPLES_PER_CLASS * generation_multiplier\n",
        "    \n",
        "    print(f\"Generating {samples_to_generate:,} samples to ensure we get enough positives...\")\n",
        "    \n",
        "    all_positive_samples = []\n",
        "    all_negative_samples = []\n",
        "    batch_size = 10000  # Generate in batches to manage memory\n",
        "    num_batches = (samples_to_generate + batch_size - 1) // batch_size\n",
        "    \n",
        "    for batch_num in range(num_batches):\n",
        "        current_batch_size = min(batch_size, samples_to_generate - batch_num * batch_size)\n",
        "        if current_batch_size <= 0:\n",
        "            break\n",
        "            \n",
        "        print(f\"  Batch {batch_num + 1}/{num_batches}: Generating {current_batch_size:,} samples...\")\n",
        "        batch = synthesizer.sample(num_rows=current_batch_size)\n",
        "        \n",
        "        # Separate by class\n",
        "        pos_batch = batch[batch['label'] == 1]\n",
        "        neg_batch = batch[batch['label'] == 0]\n",
        "        \n",
        "        all_positive_samples.append(pos_batch)\n",
        "        all_negative_samples.append(neg_batch)\n",
        "        \n",
        "        print(f\"    Found {len(pos_batch)} positives and {len(neg_batch)} negatives in this batch\")\n",
        "        \n",
        "        # Check if we have enough samples\n",
        "        total_pos = sum(len(df) for df in all_positive_samples)\n",
        "        total_neg = sum(len(df) for df in all_negative_samples)\n",
        "        \n",
        "        if total_pos >= TARGET_SAMPLES_PER_CLASS and total_neg >= TARGET_SAMPLES_PER_CLASS:\n",
        "            print(f\"  ✓ Collected enough samples! ({total_pos} positives, {total_neg} negatives)\")\n",
        "            break\n",
        "    \n",
        "    # Combine all samples\n",
        "    if all_positive_samples:\n",
        "        all_positives = pd.concat(all_positive_samples, ignore_index=True)\n",
        "    else:\n",
        "        all_positives = pd.DataFrame()\n",
        "    \n",
        "    if all_negative_samples:\n",
        "        all_negatives = pd.concat(all_negative_samples, ignore_index=True)\n",
        "    else:\n",
        "        all_negatives = pd.DataFrame()\n",
        "    \n",
        "    print(f\"\\nTotal generated: {len(all_positives)} positives, {len(all_negatives)} negatives\")\n",
        "    \n",
        "    # Take equal number from each class\n",
        "    min_samples = min(len(all_positives), len(all_negatives), TARGET_SAMPLES_PER_CLASS)\n",
        "    \n",
        "    if min_samples == 0:\n",
        "        print(\"\\n⚠️  WARNING: Could not generate any positive samples!\")\n",
        "        print(\"   This means the model learned the class imbalance too strongly.\")\n",
        "        print(\"   Solution: Retrain the model on a balanced subset of real data.\")\n",
        "        print(\"\\n   Trying alternative approach: Training on balanced data...\")\n",
        "        \n",
        "        # Alternative: Train on balanced subset\n",
        "        pos_real = data_for_generation[data_for_generation['label'] == 1]\n",
        "        neg_real = data_for_generation[data_for_generation['label'] == 0]\n",
        "        \n",
        "        # Sample equal numbers from each class for training\n",
        "        balanced_sample_size = min(len(pos_real), len(neg_real), 5000)\n",
        "        balanced_training = pd.concat([\n",
        "            pos_real.sample(n=balanced_sample_size, random_state=42),\n",
        "            neg_real.sample(n=balanced_sample_size, random_state=42)\n",
        "        ], ignore_index=True).sample(frac=1, random_state=42).reset_index(drop=True)\n",
        "        \n",
        "        print(f\"   Training new model on balanced data ({len(balanced_training)} samples)...\")\n",
        "        metadata_balanced = SingleTableMetadata()\n",
        "        metadata_balanced.detect_from_dataframe(balanced_training)\n",
        "        \n",
        "        try:\n",
        "            synthesizer_balanced = TVAESynthesizer(\n",
        "                metadata_balanced,\n",
        "                epochs=50,\n",
        "                batch_size=500,\n",
        "                verbose=True\n",
        "            )\n",
        "            synthesizer_balanced.fit(balanced_training)\n",
        "            synthesizer = synthesizer_balanced  # Use the balanced model\n",
        "            model_type = \"TVAE (balanced training)\"\n",
        "            print(\"   ✓ Balanced model trained successfully!\")\n",
        "            \n",
        "            # Now generate with balanced model\n",
        "            print(\"\\n   Generating samples with balanced model...\")\n",
        "            batch = synthesizer.sample(num_rows=TARGET_SAMPLES_PER_CLASS * 2)\n",
        "            all_positives = batch[batch['label'] == 1]\n",
        "            all_negatives = batch[batch['label'] == 0]\n",
        "            min_samples = min(len(all_positives), len(all_negatives), TARGET_SAMPLES_PER_CLASS)\n",
        "        except Exception as e:\n",
        "            print(f\"   Error training balanced model: {e}\")\n",
        "            print(\"   Please manually create a balanced training set and retrain.\")\n",
        "            synthetic_data_balanced = pd.DataFrame()\n",
        "    else:\n",
        "        # Take equal samples from each class\n",
        "        synthetic_data_balanced = pd.concat([\n",
        "            all_positives.head(min_samples),\n",
        "            all_negatives.head(min_samples)\n",
        "        ], ignore_index=True)\n",
        "        \n",
        "        # Shuffle\n",
        "        synthetic_data_balanced = synthetic_data_balanced.sample(frac=1, random_state=42).reset_index(drop=True)\n",
        "    \n",
        "    if len(synthetic_data_balanced) > 0:\n",
        "        print(f\"\\n✓ Synthetic data shape: {synthetic_data_balanced.shape}\")\n",
        "        print(f\"Class distribution in synthetic data:\")\n",
        "        print(f\"  Positives (1): {(synthetic_data_balanced['label'] == 1).sum():,} ({(synthetic_data_balanced['label'] == 1).mean()*100:.2f}%)\")\n",
        "        print(f\"  Negatives (0): {(synthetic_data_balanced['label'] == 0).sum():,} ({(synthetic_data_balanced['label'] == 0).mean()*100:.2f}%)\")\n",
        "    else:\n",
        "        print(\"\\n✗ Failed to generate balanced synthetic data.\")\n",
        "        print(\"  Please check the model training or use the balanced training approach above.\")\n",
        "else:\n",
        "    print(\"SDV library is not available. Cannot generate synthetic data.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Step 6: Visualize comparison between real and synthetic data\n",
        "if SDV_AVAILABLE:\n",
        "    fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
        "    \n",
        "    # Real data\n",
        "    real_counts = pd.Series({'No Click (0)': (y == 0).sum(), 'Click (1)': (y == 1).sum()})\n",
        "    sns.barplot(x=real_counts.index, y=real_counts.values, ax=axes[0], palette='pastel')\n",
        "    axes[0].set_title(\"Real Data - Class Distribution\")\n",
        "    axes[0].set_ylabel(\"Sample Count\")\n",
        "    for i, v in enumerate(real_counts.values):\n",
        "        axes[0].text(i, v, f\"{v:,}\", ha='center', va='bottom', fontsize=9)\n",
        "    \n",
        "    # Synthetic data\n",
        "    synth_counts = pd.Series({\n",
        "        'No Click (0)': (synthetic_data_balanced['label'] == 0).sum(),\n",
        "        'Click (1)': (synthetic_data_balanced['label'] == 1).sum()\n",
        "    })\n",
        "    sns.barplot(x=synth_counts.index, y=synth_counts.values, ax=axes[1], palette='muted')\n",
        "    axes[1].set_title(\"Synthetic Data - Class Distribution (Balanced)\")\n",
        "    axes[1].set_ylabel(\"Sample Count\")\n",
        "    for i, v in enumerate(synth_counts.values):\n",
        "        axes[1].text(i, v, f\"{v:,}\", ha='center', va='bottom', fontsize=9)\n",
        "    \n",
        "    plt.suptitle(\"Real vs Synthetic Data Class Distribution\")\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Step 7: Prepare synthetic data for model training\n",
        "if SDV_AVAILABLE:\n",
        "    # Separate features and target\n",
        "    X_synthetic = synthetic_data_balanced.drop(columns=['label'])\n",
        "    y_synthetic = synthetic_data_balanced['label']\n",
        "    \n",
        "    # Ensure same columns as original\n",
        "    X_synthetic = X_synthetic.reindex(columns=X.columns, fill_value=0)\n",
        "    \n",
        "    # Train/validation split\n",
        "    X_synth_train, X_synth_val, y_synth_train, y_synth_val = train_test_split(\n",
        "        X_synthetic, y_synthetic, test_size=0.2, stratify=y_synthetic, random_state=42\n",
        "    )\n",
        "    \n",
        "    # Scale the synthetic data\n",
        "    scaler_synth = StandardScaler()\n",
        "    num_cols = X_synth_train.select_dtypes(include=[np.number]).columns\n",
        "    X_synth_train[num_cols] = scaler_synth.fit_transform(X_synth_train[num_cols])\n",
        "    X_synth_val[num_cols] = scaler_synth.transform(X_synth_val[num_cols])\n",
        "    \n",
        "    print(f\"Synthetic training set: {X_synth_train.shape}\")\n",
        "    print(f\"Synthetic validation set: {X_synth_val.shape}\")\n",
        "    print(f\"\\nSynthetic training class distribution:\")\n",
        "    print(f\"  Positives: {y_synth_train.sum():,} ({(y_synth_train == 1).mean()*100:.2f}%)\")\n",
        "    print(f\"  Negatives: {(y_synth_train == 0).sum():,} ({(y_synth_train == 0).mean()*100:.2f}%)\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Step 8: Prepare test data (same as midterm project)\n",
        "# Note: You'll need to load and preprocess test data similar to c161.ipynb\n",
        "# For now, we'll use a validation split from the original data as a proxy\n",
        "\n",
        "print(\"Preparing test data...\")\n",
        "# Use the original validation split from real data\n",
        "X_train_real, X_test_real, y_train_real, y_test_real = train_test_split(\n",
        "    X, y, test_size=0.2, stratify=y, random_state=42\n",
        ")\n",
        "\n",
        "# Scale test data (using scaler fitted on synthetic training data)\n",
        "scaler_test = StandardScaler()\n",
        "num_cols = X_train_real.select_dtypes(include=[np.number]).columns\n",
        "X_train_real[num_cols] = scaler_test.fit_transform(X_train_real[num_cols])\n",
        "X_test_real[num_cols] = scaler_test.transform(X_test_real[num_cols])\n",
        "\n",
        "print(f\"Test set shape: {X_test_real.shape}\")\n",
        "print(f\"Test set class distribution:\")\n",
        "print(f\"  Positives: {y_test_real.sum():,} ({(y_test_real == 1).mean()*100:.2f}%)\")\n",
        "print(f\"  Negatives: {(y_test_real == 0).sum():,} ({(y_test_real == 0).mean()*100:.2f}%)\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Step 9: Train XGBoost on synthetic data\n",
        "if SDV_AVAILABLE:\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"Training XGBoost on Synthetic Data\")\n",
        "    print(\"=\"*60)\n",
        "    \n",
        "    dtrain_synth = xgb.DMatrix(X_synth_train, label=y_synth_train)\n",
        "    dval_synth = xgb.DMatrix(X_synth_val, label=y_synth_val)\n",
        "    dtest_real = xgb.DMatrix(X_test_real, label=y_test_real)\n",
        "    \n",
        "    # Note: No scale_pos_weight needed since classes are balanced\n",
        "    params_xgb_synth = {\n",
        "        'objective': 'binary:logistic',\n",
        "        'eval_metric': ['logloss', 'auc'],\n",
        "        'seed': 42,\n",
        "        'eta': 0.05,\n",
        "        'max_depth': 7,\n",
        "        'subsample': 0.8,\n",
        "        'colsample_bytree': 0.9,\n",
        "        # No scale_pos_weight - classes are balanced\n",
        "    }\n",
        "    \n",
        "    xgb_model_synth = xgb.train(\n",
        "        params_xgb_synth, \n",
        "        dtrain_synth, \n",
        "        num_boost_round=4000,\n",
        "        evals=[(dval_synth, 'val')], \n",
        "        early_stopping_rounds=100,\n",
        "        verbose_eval=False\n",
        "    )\n",
        "    \n",
        "    # Predict on test set\n",
        "    best_iter = getattr(xgb_model_synth, \"best_iteration\", None)\n",
        "    if best_iter is not None:\n",
        "        p_test_xgb_synth = xgb_model_synth.predict(dtest_real, iteration_range=(0, best_iter + 1))\n",
        "    else:\n",
        "        p_test_xgb_synth = xgb_model_synth.predict(dtest_real)\n",
        "    \n",
        "    # Evaluate\n",
        "    thr_xgb_synth = best_threshold_by_f1(y_test_real, p_test_xgb_synth)\n",
        "    results_xgb_synth = eval_at_threshold(y_test_real, p_test_xgb_synth, thr=thr_xgb_synth, label=\"XGBoost (Synthetic Data)\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Step 10: Train LightGBM on synthetic data\n",
        "if SDV_AVAILABLE:\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"Training LightGBM on Synthetic Data\")\n",
        "    print(\"=\"*60)\n",
        "    \n",
        "    train_data_synth = lgb.Dataset(X_synth_train, label=y_synth_train)\n",
        "    val_data_synth = lgb.Dataset(X_synth_val, label=y_synth_val, reference=train_data_synth)\n",
        "    \n",
        "    # Note: No scale_pos_weight needed since classes are balanced\n",
        "    params_lgb_synth = {\n",
        "        'objective': 'binary',\n",
        "        'metric': ['binary_logloss', 'auc'],\n",
        "        'verbosity': -1,\n",
        "        'boosting_type': 'gbdt',\n",
        "        'seed': 42,\n",
        "        'learning_rate': 0.05,\n",
        "        'num_leaves': 63,\n",
        "        'feature_fraction': 0.9,\n",
        "        'bagging_fraction': 0.8,\n",
        "        'bagging_freq': 1,\n",
        "        # No scale_pos_weight - classes are balanced\n",
        "    }\n",
        "    \n",
        "    lgb_model_synth = lgb.train(\n",
        "        params_lgb_synth,\n",
        "        train_data_synth,\n",
        "        valid_sets=[val_data_synth],\n",
        "        num_boost_round=4000,\n",
        "        callbacks=[lgb.early_stopping(stopping_rounds=100, verbose=False)]\n",
        "    )\n",
        "    \n",
        "    # Predict on test set\n",
        "    p_test_lgb_synth = lgb_model_synth.predict(X_test_real, num_iteration=lgb_model_synth.best_iteration)\n",
        "    \n",
        "    # Evaluate\n",
        "    thr_lgb_synth = best_threshold_by_f1(y_test_real, p_test_lgb_synth)\n",
        "    results_lgb_synth = eval_at_threshold(y_test_real, p_test_lgb_synth, thr=thr_lgb_synth, label=\"LightGBM (Synthetic Data)\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Results Comparison\n",
        "\n",
        "Compare the results from training on synthetic balanced data vs the midterm project results (trained on imbalanced real data with class weighting).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Step 11: Compare results\n",
        "if SDV_AVAILABLE:\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"RESULTS COMPARISON\")\n",
        "    print(\"=\"*60)\n",
        "    \n",
        "    # Create comparison dataframe\n",
        "    comparison_data = {\n",
        "        'Model': ['XGBoost (Synthetic)', 'LightGBM (Synthetic)'],\n",
        "        'Precision': [results_xgb_synth['precision'], results_lgb_synth['precision']],\n",
        "        'Recall': [results_xgb_synth['recall'], results_lgb_synth['recall']],\n",
        "        'F1-Score': [results_xgb_synth['f1'], results_lgb_synth['f1']],\n",
        "        'Accuracy': [results_xgb_synth['accuracy'], results_lgb_synth['accuracy']],\n",
        "        'ROC-AUC': [results_xgb_synth['roc_auc'], results_lgb_synth['roc_auc']],\n",
        "        'PR-AUC': [results_xgb_synth['pr_auc'], results_lgb_synth['pr_auc']]\n",
        "    }\n",
        "    \n",
        "    comparison_df = pd.DataFrame(comparison_data)\n",
        "    print(\"\\nFinal Project Results (Trained on Synthetic Balanced Data):\")\n",
        "    print(comparison_df.to_string(index=False))\n",
        "    \n",
        "    print(\"\\n\" + \"-\"*60)\n",
        "    print(\"NOTE: Compare these results with your midterm project results\")\n",
        "    print(\"from c161.ipynb (trained on imbalanced real data with class weighting)\")\n",
        "    print(\"-\"*60)\n",
        "    \n",
        "    # Visualize comparison\n",
        "    metrics = ['Precision', 'Recall', 'F1-Score', 'Accuracy', 'ROC-AUC', 'PR-AUC']\n",
        "    fig, axes = plt.subplots(2, 3, figsize=(18, 10))\n",
        "    axes = axes.flatten()\n",
        "    \n",
        "    for idx, metric in enumerate(metrics):\n",
        "        ax = axes[idx]\n",
        "        values = [results_xgb_synth[metric.lower().replace('-', '_')], \n",
        "                 results_lgb_synth[metric.lower().replace('-', '_')]]\n",
        "        bars = ax.bar(['XGBoost', 'LightGBM'], values, color=['skyblue', 'lightcoral'])\n",
        "        ax.set_title(f'{metric}')\n",
        "        ax.set_ylabel('Score')\n",
        "        ax.set_ylim([0, 1])\n",
        "        for i, (bar, val) in enumerate(zip(bars, values)):\n",
        "            ax.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01,\n",
        "                   f'{val:.4f}', ha='center', va='bottom', fontsize=9)\n",
        "    \n",
        "    plt.suptitle('Model Performance Metrics (Trained on Synthetic Balanced Data)', fontsize=14)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Summary and Next Steps\n",
        "\n",
        "1. **Synthetic Data Generation**: Successfully generated balanced synthetic data using CTGAN\n",
        "2. **Model Training**: Trained XGBoost and LightGBM on synthetic balanced data\n",
        "3. **Evaluation**: Tested models on the same test set as midterm project\n",
        "4. **Comparison**: Compare these results with midterm project results\n",
        "\n",
        "### Key Differences:\n",
        "- **Midterm Project**: Trained on imbalanced real data (2% clicked, 98% no-click) with class weighting\n",
        "- **Final Project**: Trained on balanced synthetic data (50% clicked, 50% no-click) without class weighting\n",
        "\n",
        "### Expected Benefits:\n",
        "- Better representation of minority class (clicked ads)\n",
        "- Potentially improved recall and PR-AUC\n",
        "- More balanced model performance\n",
        "\n",
        "### Notes:\n",
        "- The generative model may need tuning (epochs, batch size, etc.) for better synthetic data quality\n",
        "- Consider trying TVAE as an alternative to CTGAN\n",
        "- You may want to generate more synthetic data or use different sampling strategies\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
