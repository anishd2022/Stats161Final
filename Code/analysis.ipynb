{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Ads Analysis\n",
        "\n",
        "Author: Anish Deshpande\n",
        "Date: 2024-12-19\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# import necessary libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "import warnings\n",
        "import gc\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Memory monitoring for Google Colab\n",
        "def check_memory():\n",
        "    \"\"\"Check current memory usage\"\"\"\n",
        "    import psutil\n",
        "    memory = psutil.virtual_memory()\n",
        "    print(f\"Memory Usage: {memory.percent}% ({memory.used / 1024**3:.1f}GB / {memory.total / 1024**3:.1f}GB)\")\n",
        "    return memory.percent\n",
        "\n",
        "# Check initial memory\n",
        "print(\"Initial memory status:\")\n",
        "check_memory()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Read in Data\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "ename": "",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
            "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
            "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
            "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
          ]
        }
      ],
      "source": [
        "# read in training data:\n",
        "print(\"Loading training data...\")\n",
        "ads_data_train = pd.read_csv(\"../Data/train/train_data_ads.csv\")\n",
        "feeds_data_train = pd.read_csv(\"../Data/train/train_data_feeds.csv\")\n",
        "print(\"Loading test data...\")\n",
        "ads_data_test = pd.read_csv(\"../Data/test/test_data_ads.csv\")\n",
        "feeds_data_test = pd.read_csv(\"../Data/test/test_data_feeds.csv\")\n",
        "print(\"Loading codebooks...\")\n",
        "ads_codebook = pd.read_csv(\"../Data/codebooks/ads_domain_description.csv\")\n",
        "feeds_codebook = pd.read_csv(\"../Data/codebooks/feeds_domain_description.csv\")\n",
        "\n",
        "print(\"\\nData loaded successfully!\")\n",
        "print(f\"Ads train: {ads_data_train.shape}\")\n",
        "print(f\"Feeds train: {feeds_data_train.shape}\")\n",
        "print(f\"Ads test: {ads_data_test.shape}\")\n",
        "print(f\"Feeds test: {feeds_data_test.shape}\")\n",
        "\n",
        "# Check memory after loading\n",
        "print(\"\\nMemory after data loading:\")\n",
        "check_memory()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Data cleaning / Preprocessing:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Memory-efficient conversion for large datasets (Google Colab optimized)\n",
        "def convert_list_columns_chunked(df, cols, chunk_size=100000):\n",
        "    \"\"\"\n",
        "    Convert string columns with '^' separators into lists of numeric values\n",
        "    Uses chunked processing to avoid memory issues\n",
        "    \"\"\"\n",
        "    print(f\"Processing {len(df)} rows in chunks of {chunk_size}...\")\n",
        "    \n",
        "    # Process in chunks to avoid memory issues\n",
        "    chunks = []\n",
        "    for i in range(0, len(df), chunk_size):\n",
        "        chunk = df.iloc[i:i+chunk_size].copy()\n",
        "        \n",
        "        for col in cols:\n",
        "            if col in chunk.columns:\n",
        "                chunk[col] = chunk[col].astype(str).apply(lambda x: \n",
        "                    [float(val) if val.replace('.', '').replace('-', '').isdigit() else val \n",
        "                     for val in x.split('^') if val.strip() != '']\n",
        "                    if pd.notna(x) and x != 'nan' else [])\n",
        "        \n",
        "        chunks.append(chunk)\n",
        "        print(f\"Processed chunk {i//chunk_size + 1}/{(len(df)-1)//chunk_size + 1}\")\n",
        "    \n",
        "    return pd.concat(chunks, ignore_index=True)\n",
        "\n",
        "# specify the columns you want to process\n",
        "cols_to_convert = [\n",
        "    \"ad_click_list_v001\", \"ad_click_list_v002\", \"ad_click_list_v003\",\n",
        "    \"ad_close_list_v001\", \"ad_close_list_v002\", \"ad_close_list_v003\",\n",
        "    \"u_newsCatInterestsST\"\n",
        "]\n",
        "\n",
        "print(\"Converting ads training data...\")\n",
        "ads_data_train = convert_list_columns_chunked(ads_data_train, cols_to_convert)\n",
        "print(\"Converting ads test data...\")\n",
        "ads_data_test = convert_list_columns_chunked(ads_data_test, cols_to_convert)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# now do the same to the feeds training and testing data:\n",
        "cols_to_convert = [\"u_newsCatInterests\", \"u_newsCatInterestsST\", \"u_click_ca2_news\", \"i_entities\"]\n",
        "\n",
        "print(\"Converting feeds training data...\")\n",
        "feeds_data_train = convert_list_columns_chunked(feeds_data_train, cols_to_convert)\n",
        "print(\"Converting feeds test data...\")\n",
        "feeds_data_test = convert_list_columns_chunked(feeds_data_test, cols_to_convert)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Define functions for frequency encoding and one hot encoding:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def frequency_encoder(df, cols, normalize=True):\n",
        "    \"\"\"\n",
        "    Encode categorical variables using frequency encoding\n",
        "    \"\"\"\n",
        "    df_copy = df.copy()\n",
        "    \n",
        "    for col in cols:\n",
        "        if col in df_copy.columns:\n",
        "            # compute frequency table\n",
        "            freq_counts = df_copy[col].value_counts()\n",
        "            \n",
        "            if normalize:\n",
        "                freq_map = freq_counts / len(df_copy)\n",
        "            else:\n",
        "                freq_map = freq_counts\n",
        "            \n",
        "            # replace original column with frequency\n",
        "            df_copy[col] = df_copy[col].map(freq_map)\n",
        "    \n",
        "    return df_copy\n",
        "\n",
        "def one_hot_encoder(df, cols):\n",
        "    \"\"\"\n",
        "    Encode categorical variables using one-hot encoding\n",
        "    \"\"\"\n",
        "    df_copy = df.copy()\n",
        "    \n",
        "    for col in cols:\n",
        "        if col in df_copy.columns:\n",
        "            # get unique values\n",
        "            uniq_vals = df_copy[col].unique()\n",
        "            \n",
        "            # create new columns for each unique value\n",
        "            for val in uniq_vals:\n",
        "                new_col = f\"{col}_{val}\"\n",
        "                df_copy[new_col] = (df_copy[col] == val).astype(int)\n",
        "            \n",
        "            # remove original column\n",
        "            df_copy = df_copy.drop(columns=[col])\n",
        "    \n",
        "    return df_copy\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Summarize user preferences in feeds data\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def summarize_feeds_user_features(feeds_df):\n",
        "    \"\"\"\n",
        "    Summarize feeds data by user to extract user preferences\n",
        "    \"\"\"\n",
        "    \n",
        "    def top_n_from_list(lst, n=5):\n",
        "        \"\"\"Helper to extract top N most frequent values from a list column\"\"\"\n",
        "        if len(lst) == 0 or pd.isna(lst):\n",
        "            return []\n",
        "        \n",
        "        # Flatten the list of vectors into one vector\n",
        "        flat_vals = []\n",
        "        for item in lst:\n",
        "            if isinstance(item, list):\n",
        "                flat_vals.extend(item)\n",
        "            else:\n",
        "                flat_vals.append(item)\n",
        "        \n",
        "        flat_vals = [x for x in flat_vals if pd.notna(x)]\n",
        "        \n",
        "        if len(flat_vals) == 0:\n",
        "            return []\n",
        "        \n",
        "        # Count frequencies\n",
        "        from collections import Counter\n",
        "        freq_counts = Counter(flat_vals)\n",
        "        return [item for item, count in freq_counts.most_common(n)]\n",
        "    \n",
        "    # Summarize by user\n",
        "    feeds_summary = feeds_df.groupby('u_userId').agg({\n",
        "        'u_phonePrice': 'mean',\n",
        "        'u_browserLifeCycle': 'mean',\n",
        "        'u_refreshTimes': 'mean',\n",
        "        'u_newsCatInterests': lambda x: top_n_from_list(x.tolist(), 5),\n",
        "        'u_newsCatDislike': lambda x: top_n_from_list(x.tolist(), 5),\n",
        "        'u_newsCatInterestsST': lambda x: top_n_from_list(x.tolist(), 5),\n",
        "        'u_click_ca2_news': lambda x: top_n_from_list(x.tolist(), 5),\n",
        "        'label': 'mean',\n",
        "        'cillabel': 'mean',\n",
        "        'u_userId': 'count'  # feed_count\n",
        "    }).rename(columns={'u_userId': 'feed_count'})\n",
        "    \n",
        "    return feeds_summary.reset_index()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# merge feeds summary data into ads training and testing data\n",
        "feeds_summary = summarize_feeds_user_features(feeds_data_train)\n",
        "\n",
        "ads_train_enriched = ads_data_train.merge(feeds_summary,\n",
        "                                         left_on='user_id', right_on='u_userId', \n",
        "                                         how='left')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Fix data more by getting rid of vector columns:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from tqdm import tqdm\n",
        "import gc  # Garbage collection for memory management\n",
        "\n",
        "# --- Identify list-type columns ---\n",
        "list_cols = [col for col in ads_train_enriched.columns \n",
        "             if ads_train_enriched[col].apply(lambda x: isinstance(x, list)).any()]\n",
        "print(f\"List columns detected: {', '.join(list_cols)}\")\n",
        "\n",
        "def extract_fixed(x, n=5):\n",
        "    \"\"\"Extract up to n elements from each list cell\"\"\"\n",
        "    if pd.isna(x) or x is None or len(x) == 0:\n",
        "        return [0] * n\n",
        "    \n",
        "    # Truncate or pad to fixed length\n",
        "    x_list = list(x)[:n]\n",
        "    if len(x_list) < n:\n",
        "        x_list.extend([0] * (n - len(x_list)))\n",
        "    \n",
        "    return x_list\n",
        "\n",
        "# --- Memory-efficient expansion with chunked processing ---\n",
        "def expand_list_columns_chunked(df, list_cols, chunk_size=50000):\n",
        "    \"\"\"Expand list columns in chunks to avoid memory issues\"\"\"\n",
        "    print(f\"Expanding {len(list_cols)} list columns in chunks of {chunk_size}...\")\n",
        "    \n",
        "    # Process each list column one at a time to minimize memory usage\n",
        "    for col in tqdm(list_cols, desc=\"Expanding columns\"):\n",
        "        print(f\"\\nExpanding: {col}\")\n",
        "        \n",
        "        # Process in chunks\n",
        "        expanded_chunks = []\n",
        "        for i in range(0, len(df), chunk_size):\n",
        "            chunk = df.iloc[i:i+chunk_size]\n",
        "            \n",
        "            # Extract 5 numeric values per row → matrix\n",
        "            expanded_data = chunk[col].apply(lambda x: extract_fixed(x, n=5))\n",
        "            expanded_chunk = pd.DataFrame(expanded_data.tolist(), \n",
        "                                        columns=[f\"{col}_{i+1}\" for i in range(5)])\n",
        "            expanded_chunks.append(expanded_chunk)\n",
        "            \n",
        "            # Clear memory\n",
        "            del expanded_data\n",
        "            gc.collect()\n",
        "        \n",
        "        # Combine all chunks for this column\n",
        "        expanded_df = pd.concat(expanded_chunks, ignore_index=True)\n",
        "        \n",
        "        # Add to main dataframe\n",
        "        df = pd.concat([df, expanded_df], axis=1)\n",
        "        \n",
        "        # Remove the original list column\n",
        "        df = df.drop(columns=[col])\n",
        "        \n",
        "        # Clear memory\n",
        "        del expanded_df, expanded_chunks\n",
        "        gc.collect()\n",
        "        \n",
        "        print(f\"Completed {col}\")\n",
        "    \n",
        "    return df\n",
        "\n",
        "# --- Expand list columns with memory management ---\n",
        "ads_train_enriched = expand_list_columns_chunked(ads_train_enriched, list_cols)\n",
        "\n",
        "# --- Print summary of final dataset ---\n",
        "print(\"\\n✅ Expansion complete!\")\n",
        "print(f\"Final dimensions: {ads_train_enriched.shape[0]} rows x {ads_train_enriched.shape[1]} columns\")\n",
        "\n",
        "# Optional: quick check\n",
        "print(\"\\nDataset info:\")\n",
        "print(ads_train_enriched.info())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Try PCA:\n",
        "- see if we can include the vector features in PCA as well\n",
        "- scale and normalize data\n",
        "- split into training and validation sets before training on the training set\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Try Logistic Regression:\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## first scale data:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import random\n",
        "random.seed(123)  # reproducibility\n",
        "np.random.seed(123)\n",
        "\n",
        "# --- 1. Split into training (80%) and validation (20%) ---\n",
        "train_data, val_data = train_test_split(ads_train_enriched, test_size=0.2, random_state=123)\n",
        "\n",
        "# --- 2. Select numeric predictors, excluding IDs and label ---\n",
        "numeric_cols = train_data.select_dtypes(include=[np.number]).columns.tolist()\n",
        "predictor_cols = [col for col in numeric_cols if col not in [\"label\", \"user_id\", \"log_id\"]]\n",
        "\n",
        "# --- 3. Remove constant / near-constant predictors ---\n",
        "non_constant_cols = [col for col in predictor_cols \n",
        "                    if train_data[col].std() > 0]\n",
        "print(f\"Removed {len(predictor_cols) - len(non_constant_cols)} constant predictors\")\n",
        "predictor_cols = non_constant_cols\n",
        "\n",
        "# --- 4. Prepare train & validation as data.frames ---\n",
        "train_df = train_data[predictor_cols + [\"label\"]].copy()\n",
        "val_df = val_data[predictor_cols + [\"label\"]].copy()\n",
        "\n",
        "# Convert label to numeric 0/1\n",
        "train_df['label'] = train_df['label'].astype(int)\n",
        "val_df['label'] = val_df['label'].astype(int)\n",
        "\n",
        "# --- 5. Scale numeric predictors ---\n",
        "scaler = StandardScaler()\n",
        "train_df_scaled = train_df.copy()\n",
        "val_df_scaled = val_df.copy()\n",
        "\n",
        "train_df_scaled[predictor_cols] = scaler.fit_transform(train_df[predictor_cols])\n",
        "val_df_scaled[predictor_cols] = scaler.transform(val_df[predictor_cols])\n",
        "\n",
        "print(f\"Training data shape: {train_df_scaled.shape}\")\n",
        "print(f\"Validation data shape: {val_df_scaled.shape}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# --- 6. Create formula and fit logistic regression ---\n",
        "X_train = train_df_scaled[predictor_cols]\n",
        "y_train = train_df_scaled['label']\n",
        "X_val = val_df_scaled[predictor_cols]\n",
        "y_val = val_df_scaled['label']\n",
        "\n",
        "# --- 7. Fit logistic regression ---\n",
        "logit_model = LogisticRegression(random_state=123, max_iter=1000)\n",
        "logit_model.fit(X_train, y_train)\n",
        "\n",
        "# --- 8. Predict on validation data ---\n",
        "val_pred_prob = logit_model.predict_proba(X_val)[:, 1]\n",
        "val_pred_label = (val_pred_prob > 0.5).astype(int)\n",
        "\n",
        "# --- 9. Evaluate accuracy ---\n",
        "accuracy = accuracy_score(y_val, val_pred_label)\n",
        "print(f\"Validation Accuracy: {accuracy:.3f}\")\n",
        "\n",
        "# Additional metrics\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(y_val, val_pred_label))\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
