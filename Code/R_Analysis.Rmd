---
title: "Ads Analysis"
author: "Anish Deshpande"
date: "`r Sys.Date()`"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Read in Data

```{r}
# import necessary libraries
library(data.table)
library(ggplot2)
library(dplyr)
```

```{r}
# read in training data:
ads_data_train <- fread("../Data/train/train_data_ads.csv")
feeds_data_train <- fread("../Data/train/train_data_feeds.csv")
# read in testing data:
ads_data_test <- fread("../Data/test/test_data_ads.csv")
feeds_data_test <- fread("../Data/test/test_data_feeds.csv")
# read in codebooks data:
ads_codebook <- read.csv("../Data/codebooks/ads_domain_description.csv")
feeds_codebook <- read.csv("../Data/codebooks/feeds_domain_description.csv")
```

# Data cleaning / Preprocessing:

```{r}
# convert character list columns into numeric lists
convert_list_columns <- function(dt, cols, default_numeric = TRUE) {
  dt[, (cols) := lapply(.SD, function(col) {
    lapply(strsplit(col, "\\^"), function(x) {
      # Check if any element contains letters
      if (any(grepl("[A-Za-z]", x))) {
        x  # keep as character vector
      } else if (default_numeric) {
        as.numeric(x)  # convert to numeric if no letters
      } else {
        x  # keep as character if default_numeric = FALSE
      }
    })
  }), .SDcols = cols]
  return(dt)
}

# specify the columns you want to process
cols_to_convert <- c(
  "ad_click_list_v001", "ad_click_list_v002", "ad_click_list_v003",
  "ad_close_list_v001", "ad_close_list_v002", "ad_close_list_v003",
  "u_newsCatInterestsST"
)
# apply the split and numeric conversion to each column
ads_data_train <- convert_list_columns(ads_data_train, cols_to_convert)
ads_data_test <- convert_list_columns(ads_data_test, cols_to_convert)
```

```{r}
# now do the same to the feeds training and testing data:
cols_to_convert <- c("u_newsCatInterests", "u_newsCatInterestsST", "u_click_ca2_news", "i_entities")

feeds_data_train <- convert_list_columns(feeds_data_train, cols_to_convert)
feeds_data_test <- convert_list_columns(feeds_data_test, cols_to_convert)
```

## Define functions for frequency encoding and one hot encoding:

```{r}
frequency_encoder <- function(dt, cols, normalize = TRUE) {
  # dt: data.table
  # cols: vector of column names to encode
  # normalize: if TRUE, use proportion (frequency); else use counts

  for (col in cols) {
    # compute frequency table
    freq_tab <- dt[, .N, by = col]
    
    if (normalize) {
      freq_tab[, freq := N / sum(N)]
    } else {
      freq_tab[, freq := N]
    }
    
    # merge frequencies back to original dt
    dt <- merge(dt, freq_tab[, .(get(col), freq)], 
                by.x = col, by.y = col, all.x = TRUE, sort = FALSE)
    
    # replace original column with frequency
    dt[, (col) := freq]
    
    # remove temporary freq column
    dt[, freq := NULL]
  }
  
  return(dt)
}

one_hot_encoder <- function(dt, cols) {
  for (col in cols) {
    # get unique values
    uniq_vals <- unique(dt[[col]])
    
    # create new columns for each unique value
    for (val in uniq_vals) {
      new_col <- paste(col, val, sep = "_")
      dt[, (new_col) := as.integer(get(col) == val)]
    }
    
    # remove original column
    dt[, (col) := NULL]
  }
  
  return(dt)
}
```

## Summarize user preferences in feeds data 

```{r}
summarize_feeds_user_features <- function(feeds_dt) {
  # Ensure data.table
  setDT(feeds_dt)
  
  # Helper to extract top N most frequent values from a list column
  top_n_from_list <- function(lst, n = 5) {
    if (length(lst) == 0) return(NA_character_)
    
    # Flatten the list of vectors into one vector
    flat_vals <- unlist(lst, use.names = FALSE)
    flat_vals <- flat_vals[!is.na(flat_vals)]
    
    if (length(flat_vals) == 0) return(NA_character_)
    
    # Efficient counting using data.table
    freq_dt <- data.table(value = flat_vals)[, .N, by = value][order(-N)]
    return(head(freq_dt$value, n))
  }
  
  # Summarize by user
  feeds_summary <- feeds_dt[, .(
    mean_phone_price        = mean(u_phonePrice, na.rm = TRUE),
    mean_browser_lifecycle   = mean(u_browserLifeCycle, na.rm = TRUE),
    mean_refresh_times       = mean(u_refreshTimes, na.rm = TRUE),
    
    # top 5 most frequent values across each list column
    top_news_cat_interests   = list(top_n_from_list(u_newsCatInterests, 5)),
    top_news_cat_dislike     = list(top_n_from_list(u_newsCatDislike, 5)),
    top_news_cat_interestsST = list(top_n_from_list(u_newsCatInterestsST, 5)),
    top_click_ca2_news       = list(top_n_from_list(u_click_ca2_news, 5)),
    
    mean_label               = mean(label, na.rm = TRUE),
    mean_cillabel            = mean(cillabel, na.rm = TRUE),
    feed_count               = .N
  ), by = u_userId]
  
  return(feeds_summary)
}
```

```{r}
# merge feeds summary data into ads training and testing data
feeds_summary <- summarize_feeds_user_features(feeds_data_train)

ads_train_enriched <- merge(ads_data_train, feeds_summary,
                             by.x = "user_id", by.y = "u_userId", all.x = TRUE)
```

# Fix data more by getting rid of vector columns:

```{r}
library(progress)

# --- Identify list-type columns ---
list_cols <- names(ads_train_enriched)[sapply(ads_train_enriched, is.list)]
cat("List columns detected:", paste(list_cols, collapse = ", "), "\n")

# --- Function to safely extract up to n elements from each list cell ---
extract_fixed <- function(x, n = 5) {
  # Convert from comma-separated string to numeric if needed
  if (is.character(x)) {
    x <- suppressWarnings(as.numeric(unlist(strsplit(x, ","))))
  }
  
  # Handle NULL, NA, or empty
  if (is.null(x) || length(x) == 0 || all(is.na(x))) return(rep(0, n))
  
  # Truncate or pad to fixed length
  x <- x[seq_len(min(length(x), n))]
  if (length(x) < n) x <- c(x, rep(0, n - length(x)))
  
  return(x)
}

# --- Progress bar setup ---
pb <- progress_bar$new(
  format = "  Expanding [:bar] :percent  ETA: :eta",
  total = length(list_cols),
  clear = FALSE,
  width = 60
)

# --- Expand each list column ---
for (col in list_cols) {
  pb$tick()  # update progress bar
  cat("\nExpanding:", col, "\n")
  
  # Extract 5 numeric values per row → matrix
  expanded_mat <- t(sapply(ads_train_enriched[[col]], extract_fixed, n = 5))
  
  # Create new column names
  new_colnames <- paste0(col, "_", 1:5)
  expanded_dt <- as.data.table(expanded_mat)
  setnames(expanded_dt, new_colnames)
  
  # Bind new columns into main data.table
  ads_train_enriched <- cbind(ads_train_enriched, expanded_dt)
  
  # Remove the original list column
  ads_train_enriched[, (col) := NULL]
}

# --- Print summary of final dataset ---
cat("\n✅ Expansion complete!\n")
cat("Final dimensions:", dim(ads_train_enriched)[1], "rows x", dim(ads_train_enriched)[2], "columns\n")

# Optional: quick check
str(ads_train_enriched, list.len = 10)
```


```{r}
# --- Refactor List Columns to Individual Elements ---
# Convert list columns to separate columns for first 3 elements
# If list has < 3 elements, fill missing with 0
# If list has > 3 elements, ignore the rest

# Function to extract first 3 elements from a list column
extract_first_three <- function(list_col) {
  # Handle different input types - check for null/NA more carefully
  if (length(list_col) == 0 || all(is.na(list_col)) || 
      (length(list_col) == 1 && (is.null(list_col) || is.na(list_col) || list_col == ""))) {
    return(c(0, 0, 0))
  }
  
  # If it's already a numeric vector, use it directly
  if (is.numeric(list_col) && length(list_col) > 0) {
    numeric_elements <- list_col[!is.na(list_col)]
    result <- rep(0, 3)
    if (length(numeric_elements) > 0) {
      result[1:min(3, length(numeric_elements))] <- numeric_elements[1:min(3, length(numeric_elements))]
    }
    return(result)
  }
  
  # Convert to character and clean
  list_str <- as.character(list_col)
  if (length(list_str) == 0 || all(list_str == "") || all(list_str == "[]")) {
    return(c(0, 0, 0))
  }
  
  # If we have multiple strings, join them with comma
  if (length(list_str) > 1) {
    list_str <- paste(list_str, collapse = ",")
  }
  
  # Split by comma and clean
  elements <- strsplit(list_str, ",")[[1]]
  elements <- trimws(elements)
  elements <- elements[elements != ""]
  
  # Convert to numeric, handling non-numeric values
  numeric_elements <- suppressWarnings(as.numeric(elements))
  numeric_elements <- numeric_elements[!is.na(numeric_elements)]
  
  # Return first 3 elements, padding with 0s if needed
  result <- rep(0, 3)
  if (length(numeric_elements) > 0) {
    result[1:min(3, length(numeric_elements))] <- numeric_elements[1:min(3, length(numeric_elements))]
  }
  
  return(result)
}

# Identify list columns that need refactoring
list_columns_to_refactor <- c(
  "u_newsCatInterestsST",
  "top_news_cat_interests", 
  "top_news_cat_dislike",
  "top_news_cat_interestsST",
  "top_click_ca2_news"
)

cat("Refactoring list columns to individual elements...\n")

# Process each list column
for (col in list_columns_to_refactor) {
  if (col %in% names(ads_train_enriched)) {
    cat("Processing:", col, "\n")
    
    # Extract first 3 elements for each row
    expanded_data <- t(sapply(ads_train_enriched[[col]], extract_first_three))
    
    # Create new column names
    new_colnames <- paste0(col, "_", 1:3)
    
    # Add new columns using standard data.frame operations
    for (i in 1:3) {
      ads_train_enriched[[new_colnames[i]]] <- expanded_data[, i]
    }
    
    # Remove original list column
    ads_train_enriched[[col]] <- NULL
    
    cat("  Created columns:", paste(new_colnames, collapse = ", "), "\n")
  } else {
    cat("  Column", col, "not found in dataset\n")
  }
}

cat("\n✅ List column refactoring complete!\n")
cat("New dimensions:", dim(ads_train_enriched)[1], "rows x", dim(ads_train_enriched)[2], "columns\n")

# Quick verification - show first few rows of new columns
cat("\nSample of refactored columns:\n")
new_cols <- names(ads_train_enriched)[grepl("_1$|_2$|_3$", names(ads_train_enriched))]
if (length(new_cols) > 0) {
  print(head(ads_train_enriched[, ..new_cols], 3))
}
```

```{r}
# write csv for ads train enriched:
write.csv(ads_train_enriched, "ads_train_enriched.csv", row.names = FALSE)
```

# Try PCA:
- see if we can include the vector features in PCA as well
- scale and normalize data
- split into training and validation sets before training on the training set








# Try Logistic Regression:

## first scale data:

```{r}
library(biglm)
set.seed(123)  # reproducibility

# --- 1. Split into training (80%) and validation (20%) ---
train_indices <- sample(seq_len(nrow(ads_train_enriched)), size = 0.8 * nrow(ads_train_enriched))
train_data <- ads_train_enriched[train_indices, ]
val_data   <- ads_train_enriched[-train_indices, ]

# --- 2. Select numeric predictors, excluding IDs and label ---
numeric_cols <- names(train_data)[sapply(train_data, is.numeric)]
predictor_cols <- setdiff(numeric_cols, c("label", "user_id", "log_id"))

# --- 3. Remove constant / near-constant predictors ---
non_constant_cols <- predictor_cols[sapply(train_data[, predictor_cols, with = FALSE], 
                                           function(x) sd(x, na.rm = TRUE) > 0)]
cat("Removed", length(predictor_cols) - length(non_constant_cols), "constant predictors\n")
predictor_cols <- non_constant_cols

# --- 4. Prepare train & validation as data.frames ---
train_df <- as.data.frame(train_data[, c(predictor_cols, "label"), with = FALSE])
val_df   <- as.data.frame(val_data[, c(predictor_cols, "label"), with = FALSE])

# Convert label to numeric 0/1
train_df$label <- as.numeric(as.character(train_df$label))
val_df$label   <- as.numeric(as.character(val_df$label))

# --- 5. Scale numeric predictors ---
scaling_params <- lapply(train_df[, predictor_cols], function(x) list(mean = mean(x, na.rm = TRUE), sd = sd(x, na.rm = TRUE)))
for(col in predictor_cols){
  train_df[[col]] <- (train_df[[col]] - scaling_params[[col]]$mean) / scaling_params[[col]]$sd
  val_df[[col]]   <- (val_df[[col]] - scaling_params[[col]]$mean) / scaling_params[[col]]$sd
}

# --- 6. Create formula ---
model_formula <- as.formula(paste("label ~", paste(predictor_cols, collapse = " + ")))

# --- 7. Fit logistic regression with bigglm ---
logit_big <- bigglm(
  formula = model_formula,
  data = train_df,
  family = binomial(),
  chunksize = 50000
)

# --- 8. Predict on validation data ---
val_pred_prob <- predict(logit_big, newdata = val_df, type = "response")
val_pred_label <- ifelse(val_pred_prob > 0.5, 1, 0)

# --- 9. Evaluate accuracy ---
accuracy <- mean(val_pred_label == val_df$label)
cat("Validation Accuracy:", round(accuracy, 3), "\n")
```

```{r}

```

```{r}
# write csv for ads train enriched:
write.csv(ads_train_enriched, "ads_train_enriched.csv", row.names = FALSE)
```


















